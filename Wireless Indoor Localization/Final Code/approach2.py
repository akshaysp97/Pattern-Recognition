# -*- coding: utf-8 -*-
"""Approach2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16l25G4pNs84XwMOuNMRGW6vVew0FpFM9
"""

#Approach 2
"""
In this approach we operate on new features generated as nonlinear functions of the given 7 features(nonlinear mapping) 
where we train and classify data based on the best 40 features selected through feature ranking 
with recursive feature elimination done in feature_generation_select.py

"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from feature_generation import get_features
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

#Read Training data
df = pd.read_csv('D_Train1.csv')
X = df.drop(columns = ['Location'])
y = df['Location']

X = get_features(X)

selected = ['WS1',
 'WS2',
 'WS3',
 'WS4',
 'WS5',
 'WS6',
 'WS7',
 'mean',
 'std',
 'WS1_scaled',
 'WS2_scaled',
 'WS3_scaled',
 'WS4_scaled',
 'WS5_scaled',
 'WS1_max_ws',
 'WS1_max_ws_dist',
 'WS1_scaled_max_ws_dist',
 'WS3_scaled_max_ws_dist',
 'WS5_scaled_max_ws_dist',
 'WS1_min_ws_dist',
 'WS4_min_ws_dist',
 'WS7_min_ws_dist',
 'mean_min_ws_dist',
 'std_min_ws_dist',
 'WS1_scaled_min_ws_dist',
 'WS2_scaled_min_ws_dist',
 'WS4_scaled_min_ws_dist',
 'WS5_scaled_min_ws_dist',
 'WS7_scaled_min_ws_dist',
 'WS2_max_ws_min_ws_dist',
 'WS4_scaled_max_ws_min_ws_dist',
 'WS5_scaled_max_ws_min_ws_dist',
 'WS3_max_ws_dist_min_ws_dist',
 'WS6_scaled_max_ws_dist_min_ws_dist',
 'WS1_scaled_min_max_dist',
 'WS5_max_ws_dist_min_max_dist',
 'mean_max_ws_dist_min_max_dist',
 'WS2_scaled_max_ws_dist_min_max_dist',
 'WS3_scaled_max_ws_dist_min_ws_dist_min_max_dist',
 'WS5_scaled_max_ws_dist_min_ws_dist_min_max_dist']

X = X[selected]

#Split arrays or matrices into random train and validation subsets
X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.2, random_state = 0,stratify=y)

#Standardization of the Dataset
x_scaler = StandardScaler()
X_train = x_scaler.fit_transform(X_train)
X_val = x_scaler.transform(X_val)
X_train.shape

#Function to plot Confusion Matrix
def plot_conf(c_mat):
    plt.figure()                    #Citation - from matplotlib.org matplotlip API reference documentation
    ax = sns.heatmap(c_mat,annot=True,fmt='d',cmap="YlGnBu")       ##Citation - seaborn.pydata.org seaborn documentation
    plt.title('Confusion Matrix')  
    plt.xlabel('Predicted')
    plt.ylabel('True')

#Baseline System Naive Bayes
nb = GaussianNB()                   #Citation - scikit-learn.org sklearn API reference documentation
nb.fit(X_train, y_train)
y_pred = nb.predict(X_val)

print("Training Accuracy:", nb.score(X_train, y_train))
print("Validation Set Accuracy:", nb.score(X_val, y_val))      
cross_val = cross_val_score(nb, x_scaler.transform(X), y, cv=5)       #Citation - scikit-learn.org sklearn API reference documentation
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))          #Citation - scipy.org numpy documentation
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)        #Citation - scikit-learn.org sklearn API reference documentation
plot_conf(conf_mat)

#Logistic Regression classifier.
c = [0.001,0.01, 0.1, 1, 10, 50, 100, 200]                #Citation - scipy.org numpy documentation
param_grid = {'C':c}

grid_search = GridSearchCV(LogisticRegression(),param_grid,cv=5,n_jobs=-1)           #Citation - scikit-learn.org sklearn API reference documentation
grid_result = grid_search.fit(X_train,y_train)

print("The best parameter:", grid_result.best_params_)

lr = LogisticRegression(C = grid_result.best_params_['C'], penalty = 'l2',       
                        solver='lbfgs',multi_class='multinomial', n_jobs = -1)          #Citation - scikit-learn.org sklearn API reference documentation
lr_fit = lr.fit(X_train,y_train)
y_pred = lr.predict(X_val)

print("Training Accuracy:", lr.score(X_train, y_train))
print("Validation Accuracy:", lr.score(X_val, y_val))
cross_val = cross_val_score(lr, x_scaler.transform(X), y, cv=5)                       #Citation - scikit-learn.org sklearn API reference documentation
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))                         
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)                      #Citation - scikit-learn.org sklearn API reference documentation
plot_conf(conf_mat)

#SVM classifier
c = [0.01, 0.1, 1, 10, 50, 100, 200]
gamma = [0.001, 0.01, 0.1, 1, 10]

param_grid = {'C':c, 'gamma':gamma}

grid_search = GridSearchCV(SVC(),param_grid,cv=5,n_jobs=-1)                                 #Citation - scikit-learn.org sklearn API reference documentation
grid_result = grid_search.fit(X_train,y_train)

print("The best parameters:", grid_result.best_params_)

svc = SVC(kernel = 'rbf', C = grid_result.best_params_['C'],                      #Citation - scikit-learn.org sklearn API reference documentation
          gamma = grid_result.best_params_['gamma'], probability=True)  
svc.fit(X_train,y_train)
y_pred = svc.predict(X_val)

print("Training Accuracy:", svc.score(X_train, y_train))
print("Validation Accuracy:", svc.score(X_val, y_val))
cross_val = cross_val_score(svc, x_scaler.transform(X), y, cv=5)
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)                      #Citation - scikit-learn.org sklearn API reference documentation
plot_conf(conf_mat)

#XGB Classififer
rates = [0.01,0.05,0.1,0.15,0.2]
n = [10, 50, 100, 200]
depth_values = range(1,20,1)

param_grid  = {'learning_rate':rates,'n_estimators':n, 'max_depth':depth_values, 'random_state':[0]} 

grid_search = GridSearchCV(XGBClassifier(),param_grid,cv=5)                             #Citation - scikit-learn.org sklearn API reference documentation
grid_result = grid_search.fit(X_train,y_train)

print("The best parameters:",grid_result.best_params_)
xgb = XGBClassifier(n_estimators=grid_result.best_params_['n_estimators'],                      #Citation - scikit-learn.org sklearn API reference documentation
                                learning_rate = grid_result.best_params_['learning_rate'],
                                max_depth = grid_result.best_params_['max_depth'],
                                objective='multi:softmax', n_jobs = -1, random_state=0)

xgb.fit(X_train,y_train)
y_pred = xgb.predict(X_val)

print("Training Accuracy:", xgb.score(X_train, y_train))
print("Validation Accuracy:", xgb.score(X_val, y_val))
cross_val = cross_val_score(xgb, x_scaler.transform(X), y, cv=5)
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)                #Citation - scikit-learn.org sklearn API reference documentation
plot_conf(conf_mat)

#KNN Classifier
k = list(range(1,11))
weights = ['uniform','distance']

param_grid = {'n_neighbors':k,'weights': weights}
grid_search = GridSearchCV(KNeighborsClassifier(),param_grid,cv=5,n_jobs=-1)             #Citation - scikit-learn.org sklearn API reference documentation
grid_result = grid_search.fit(X_train,y_train)

print("The best parameters:", grid_result.best_params_)

knn = KNeighborsClassifier(n_neighbors = grid_result.best_params_['n_neighbors'],             #Citation - scikit-learn.org sklearn API reference documentation
                           weights = grid_result.best_params_['weights'], n_jobs = -1)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_val)

print("Training Accuracy:", knn.score(X_train, y_train))
print("Validation Accuracy:", knn.score(X_val, y_val))
cross_val = cross_val_score(knn, x_scaler.transform(X), y, cv=5)
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)
plot_conf(conf_mat)

#Random Forest Classifier
max_depth = list(range(1,20,1))
n = [10, 50, 100, 200]

param_grid = {'max_depth':max_depth,'n_estimators':n,'random_state':[0]}          
grid_search = GridSearchCV(RandomForestClassifier(),param_grid,cv=5,n_jobs=-1)                   #Citation - scikit-learn.org sklearn API reference documentation
grid_result = grid_search.fit(X_train,y_train)

print("The best parameters:",grid_result.best_params_)

rf = RandomForestClassifier(n_estimators = grid_result.best_params_['n_estimators'],      #Citation - scikit-learn.org sklearn API reference documentation
                            max_depth = grid_result.best_params_['max_depth'],
                            n_jobs = -1, random_state=0)
rf.fit(X_train,y_train)
y_pred = rf.predict(X_val)

print("Training Accuracy:", rf.score(X_train, y_train))
print("Validation Data Accuracy:", rf.score(X_val, y_val))
cross_val = cross_val_score(rf, x_scaler.transform(X), y, cv=5)
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)
plot_conf(conf_mat)

#MLP Classifier
alpha = 10.0 ** -np.arange(1, 10)
max_iter = [800,1000,1500]

param_grid = {'alpha':alpha,'max_iter':max_iter,'random_state':[0]}          
grid_search = GridSearchCV(MLPClassifier(),param_grid,cv=5)                   #Citation - scikit-learn.org sklearn API reference documentation
grid_result = grid_search.fit(X_train,y_train)

print("The best parameters:",grid_result.best_params_)

mlp = MLPClassifier(alpha = grid_result.best_params_['alpha'],                    #Citation - scikit-learn.org sklearn API reference documentation
                    max_iter = grid_result.best_params_['max_iter'],random_state=0)
mlp.fit(X_train,y_train)
y_pred = mlp.predict(X_val)

print("Training Accuracy:", mlp.score(X_train, y_train))
print("Validation Accuracy:", mlp.score(X_val, y_val))
cross_val = cross_val_score(mlp, x_scaler.transform(X), y, cv=5)
print("Mean Cross-Validation Accuracy:", np.mean(cross_val))
print("Standard deviation:", np.std(cross_val))

#Confusion matrix for Validation Data
conf_mat = confusion_matrix(y_val, y_pred)
plot_conf(conf_mat)

#Read Test data
df_test = pd.read_csv('D_Test1.csv')
X_test = df_test.drop(columns = ['Location'])
y_test = df_test['Location']

X_test = get_features(X_test)

X_test = x_scaler.transform(X_test[selected])

print("Test Accuracy:" , mlp.score(X_test,y_test))

#Confusion Matrix for Test Data
y_pred = mlp.predict(X_test)
conf_test = confusion_matrix(y_test, y_pred)           #Citation - scikit-learn.org sklearn API reference documentation
plot_conf(conf_test)